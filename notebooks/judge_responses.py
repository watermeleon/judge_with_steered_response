#!/usr/bin/env python3
"""
Judge responses generated by feedback_steering.py for sycophancy evaluation.
"""

import os
import sys

# Move one directory up
parent_dir = os.path.dirname(os.getcwd())
# os.chdir(parent_dir)
sys.path.append(parent_dir)

import json
import argparse
import datetime
from pathlib import Path
from tqdm import tqdm
from openai import OpenAI
import numpy as np
from notebooks.openai_judge_json import evaluate_sycophancy_pair, evaluate_sycophancy_single


def setup_environment():
    """Set up the environment by adding parent directory to path."""
    parent_dir = os.path.dirname(os.getcwd())
    os.chdir(parent_dir)
    sys.path.append(parent_dir)
    print(f"Moved to parent directory: {parent_dir}")


def load_responses(json_file):
    """
    Load responses from a JSON file.
    
    Args:
        json_file (str): Path to the JSON file
        
    Returns:
        tuple: (settings, responses) tuple
    """
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    settings = data.get('settings', {})
    responses = data.get('responses', [])
    
    print(f"Loaded {len(responses)} responses from {json_file}")
    print(f"Settings: {settings}")
    
    return settings, responses


def initialize_openai_client(api_key=None):
    """
    Initialize the OpenAI client with the provided API key.
    
    Args:
        api_key (str, optional): OpenAI API key. If None, uses environment variable.
        
    Returns:
        OpenAI: OpenAI client
    """
    if api_key is None:
        api_key = os.environ.get("OPENAI_API_KEY", "")
        
    if not api_key:
        raise ValueError("No OpenAI API key provided or found in environment")
        
    return OpenAI(api_key=api_key)


def judge_responses_single(responses, client, model_slug, prompt_only=False):
    """
    Judge responses using individual evaluation.
    
    Args:
        responses (list): List of response dictionaries
        client (OpenAI): OpenAI client
        model_slug (str): Model to use for judging
        prompt_only (bool): Whether this is prompt-only mode
        
    Returns:
        list: Updated responses with judge results
    """
    for item in tqdm(responses, desc="Judging individual responses"):
        result_individual = {}
        
        # For prompt-only mode, we only have suggestive responses
        if prompt_only:
            user_question = item['suggestive_prompt']
            
            # Judge the suggestive response
            single_pair = {
                'user_question': user_question,
                'first_comment': item['suggestive_response']
            }
            suggestive_result = evaluate_sycophancy_single(client, model_slug, single_pair)
            result_individual["suggestive_response_score"] = suggestive_result['sycophant_score_response']
            
            # Judge the steered suggestive response if available
            if 'suggestive_steered_response' in item:
                single_pair['first_comment'] = item['suggestive_steered_response']
                suggestive_steered_result = evaluate_sycophancy_single(client, model_slug, single_pair)
                result_individual["suggestive_steered_response_score"] = suggestive_steered_result['sycophant_score_response']
        
        # For regular mode, we have both base and suggestive responses
        else:
            user_question = item['base_prompt']
            
            # Judge the suggestive response
            single_pair = {
                'user_question': user_question,
                'first_comment': item['suggestive_response']
            }
            suggestive_result = evaluate_sycophancy_single(client, model_slug, single_pair)
            result_individual["suggestive_response_score"] = suggestive_result['sycophant_score_response']
            
            # Judge the base response
            single_pair['first_comment'] = item['base_response']
            base_result = evaluate_sycophancy_single(client, model_slug, single_pair)
            result_individual["base_response_score"] = base_result['sycophant_score_response']
            
            # Judge steered responses if available
            if 'base_steered_response' in item:
                single_pair['first_comment'] = item['base_steered_response']
                base_steered_result = evaluate_sycophancy_single(client, model_slug, single_pair)
                result_individual["base_steered_response_score"] = base_steered_result['sycophant_score_response']
                
            if 'suggestive_steered_response' in item:
                single_pair['first_comment'] = item['suggestive_steered_response']
                suggestive_steered_result = evaluate_sycophancy_single(client, model_slug, single_pair)
                result_individual["suggestive_steered_response_score"] = suggestive_steered_result['sycophant_score_response']
        
        item['judge_individual'] = result_individual
    
    return responses


def judge_responses_paired(responses, client, model_slug, prompt_only=False):
    """
    Judge responses using paired evaluation.
    
    Args:
        responses (list): List of response dictionaries
        client (OpenAI): OpenAI client
        model_slug (str): Model to use for judging
        prompt_only (bool): Whether this is prompt-only mode
        
    Returns:
        list: Updated responses with judge results
    """
    for item in tqdm(responses, desc="Judging paired responses"):
        # For prompt-only mode, we only compare steered vs unsteered suggestive responses
        if prompt_only:
            # Skip if there's no steered response
            if 'suggestive_steered_response' not in item:
                continue
                
            user_question = item['suggestive_prompt']
            
            # Compare suggestive with steered suggestive
            pair = {
                "first_comment": item['suggestive_response'],
                "second_comment": item['suggestive_steered_response'],
                "user_question": user_question
            }
            result = evaluate_sycophancy_pair(client, model_slug, pair)
            
            # Store results
            result_paired = {
                "suggestive_response_score": result['response_a_sycophantic'],
                "suggestive_steered_response_score": result['response_b_sycophantic'],
                'best_answer': result['preferred_response']
            }
            item['judge_steered_pair'] = result_paired
        
        # For regular mode, we have multiple comparison options
        else:
            user_question = item['base_prompt']
            
            # Compare base with suggestive
            pair = {
                "first_comment": item['base_response'],
                "second_comment": item['suggestive_response'],
                "user_question": user_question
            }
            result = evaluate_sycophancy_pair(client, model_slug, pair)
            
            # Store results
            result_paired = {
                "base_response_score": result['response_a_sycophantic'],
                "suggestive_response_score": result['response_b_sycophantic'],
                'best_answer': result['preferred_response']
            }
            item['judge_paired'] = result_paired
            
            # If steered responses exist, compare them too
            if 'base_steered_response' in item and 'suggestive_steered_response' in item:
                # Compare base with base_steered
                pair = {
                    "first_comment": item['base_response'],
                    "second_comment": item['base_steered_response'],
                    "user_question": user_question
                }
                result = evaluate_sycophancy_pair(client, model_slug, pair)
                
                # Store results
                result_paired = {
                    "base_response_score": result['response_a_sycophantic'],
                    "base_steered_response_score": result['response_b_sycophantic'],
                    'best_answer': result['preferred_response']
                }
                item['judge_base_steered'] = result_paired
                
                # Compare suggestive with suggestive_steered
                pair = {
                    "first_comment": item['suggestive_response'],
                    "second_comment": item['suggestive_steered_response'],
                    "user_question": user_question
                }
                result = evaluate_sycophancy_pair(client, model_slug, pair)
                
                # Store results
                result_paired = {
                    "suggestive_response_score": result['response_a_sycophantic'],
                    "suggestive_steered_response_score": result['response_b_sycophantic'],
                    'best_answer': result['preferred_response']
                }
                item['judge_suggestive_steered'] = result_paired
                
                # Compare base_steered with suggestive_steered
                pair = {
                    "first_comment": item['base_steered_response'],
                    "second_comment": item['suggestive_steered_response'],
                    "user_question": user_question
                }
                result = evaluate_sycophancy_pair(client, model_slug, pair)
                
                # Store results
                result_paired = {
                    "base_steered_response_score": result['response_a_sycophantic'],
                    "suggestive_steered_response_score": result['response_b_sycophantic'],
                    'best_answer': result['preferred_response']
                }
                item['judge_steered_pair'] = result_paired
    
    return responses


def create_score_stats(scores):
    """
    Create consistent statistics dictionary for a list of scores.
    
    Args:
        scores (list): List of numeric scores
        
    Returns:
        dict: Dictionary with consistent statistics
    """
    if not scores:
        return None
    
    return {
        'mean': float(np.mean(scores)),
        'std': float(np.std(scores)),
        'min': float(np.min(scores)),
        'max': float(np.max(scores)),
        'count': len(scores)
    }


def compute_summary_statistics(responses, prompt_only=False):
    """
    Compute summary statistics from judged responses.
    
    Args:
        responses (list): List of response dictionaries with judge results
        prompt_only (bool): Whether this is prompt-only mode
        
    Returns:
        dict: Summary statistics
    """
    summary = {
        'individual': {},
        'paired': {},
        'comparisons': {}
    }
    
    # Individual scores
    if prompt_only:
        # For prompt-only, only compute suggestive scores
        suggestive_scores = [r['judge_individual'].get('suggestive_response_score', 0) for r in responses 
                             if 'judge_individual' in r and 'suggestive_response_score' in r['judge_individual']]
        if suggestive_scores:
            summary['individual']['suggestive'] = create_score_stats(suggestive_scores)
        
        # If steered responses exist, compute their scores too
        steered_scores = [r['judge_individual'].get('suggestive_steered_response_score', 0) for r in responses 
                         if 'judge_individual' in r and 'suggestive_steered_response_score' in r['judge_individual']]
        if steered_scores:
            summary['individual']['suggestive_steered'] = create_score_stats(steered_scores)
            
            # Compute improvement
            if len(suggestive_scores) == len(steered_scores):
                improvements = [steered - original for steered, original in zip(steered_scores, suggestive_scores)]
                improvement_stats = create_score_stats(improvements)
                if improvement_stats:
                    improvement_stats['positive_percent'] = sum(1 for imp in improvements if imp < 0) / len(improvements) * 100
                    summary['individual']['improvement'] = improvement_stats
    else:
        # For regular mode, compute both base and suggestive scores
        base_scores = [r['judge_individual'].get('base_response_score', 0) for r in responses 
                       if 'judge_individual' in r and 'base_response_score' in r['judge_individual']]
        suggestive_scores = [r['judge_individual'].get('suggestive_response_score', 0) for r in responses 
                            if 'judge_individual' in r and 'suggestive_response_score' in r['judge_individual']]
        
        if base_scores:
            summary['individual']['base'] = create_score_stats(base_scores)
        
        if suggestive_scores:
            summary['individual']['suggestive'] = create_score_stats(suggestive_scores)
        
        # Compute sycophancy gap
        if base_scores and suggestive_scores and len(base_scores) == len(suggestive_scores):
            gaps = [suggestive - base for suggestive, base in zip(suggestive_scores, base_scores)]
            gap_stats = create_score_stats(gaps)
            if gap_stats:
                gap_stats['positive_percent'] = sum(1 for gap in gaps if gap > 0) / len(gaps) * 100
                summary['individual']['sycophancy_gap'] = gap_stats
        
        # If steered responses exist, compute their scores too
        base_steered_scores = [r['judge_individual'].get('base_steered_response_score', 0) for r in responses 
                              if 'judge_individual' in r and 'base_steered_response_score' in r['judge_individual']]
        suggestive_steered_scores = [r['judge_individual'].get('suggestive_steered_response_score', 0) for r in responses 
                                    if 'judge_individual' in r and 'suggestive_steered_response_score' in r['judge_individual']]
        
        if base_steered_scores:
            summary['individual']['base_steered'] = create_score_stats(base_steered_scores)
        
        if suggestive_steered_scores:
            summary['individual']['suggestive_steered'] = create_score_stats(suggestive_steered_scores)
            
        # Compute steered sycophancy gap
        if base_steered_scores and suggestive_steered_scores and len(base_steered_scores) == len(suggestive_steered_scores):
            steered_gaps = [suggestive - base for suggestive, base in zip(suggestive_steered_scores, base_steered_scores)]
            steered_gap_stats = create_score_stats(steered_gaps)
            if steered_gap_stats:
                steered_gap_stats['positive_percent'] = sum(1 for gap in steered_gaps if gap > 0) / len(steered_gaps) * 100
                summary['individual']['steered_sycophancy_gap'] = steered_gap_stats
    
    # Paired comparisons
    if prompt_only:
        # For prompt-only, only check steered vs unsteered
        steered_preferred = [r['judge_steered_pair']['best_answer'] == 'B' for r in responses 
                             if 'judge_steered_pair' in r]
        if steered_preferred:
            summary['paired']['steered_preferred_percent'] = sum(steered_preferred) / len(steered_preferred) * 100
    else:
        # For regular mode, check base vs suggestive
        suggestive_preferred = [r['judge_paired']['best_answer'] == 'B' for r in responses 
                               if 'judge_paired' in r]
        if suggestive_preferred:
            summary['paired']['suggestive_preferred_percent'] = sum(suggestive_preferred) / len(suggestive_preferred) * 100
        
        # Check base vs base_steered
        base_steered_preferred = [r['judge_base_steered']['best_answer'] == 'B' for r in responses 
                                 if 'judge_base_steered' in r]
        if base_steered_preferred:
            summary['paired']['base_steered_preferred_percent'] = sum(base_steered_preferred) / len(base_steered_preferred) * 100
        
        # Check suggestive vs suggestive_steered
        suggestive_steered_preferred = [r['judge_suggestive_steered']['best_answer'] == 'B' for r in responses 
                                       if 'judge_suggestive_steered' in r]
        if suggestive_steered_preferred:
            summary['paired']['suggestive_steered_preferred_percent'] = sum(suggestive_steered_preferred) / len(suggestive_steered_preferred) * 100
        
        # Check base_steered vs suggestive_steered
        steered_suggestive_preferred = [r['judge_steered_pair']['best_answer'] == 'B' for r in responses 
                                       if 'judge_steered_pair' in r]
        if steered_suggestive_preferred:
            summary['paired']['steered_suggestive_preferred_percent'] = sum(steered_suggestive_preferred) / len(steered_suggestive_preferred) * 100
    
    return summary


def print_summary(summary, prompt_only=False):
    """
    Print summary statistics in a readable format.
    
    Args:
        summary (dict): Summary statistics
        prompt_only (bool): Whether this is prompt-only mode
    """
    print("\n===== SUMMARY STATISTICS =====\n")
    
    print("Individual Scores:")
    for key, value in summary['individual'].items():
        if not value:  # Skip empty stats
            continue
            
        if 'mean' in value and 'min' in value and 'max' in value:
            print(f"  {key.replace('_', ' ').title()}: Mean = {value['mean']:.2f}, Std = {value['std']:.2f}, Range = [{value['min']:.2f} - {value['max']:.2f}]")
        elif 'mean' in value and 'positive_percent' in value:
            print(f"  {key.replace('_', ' ').title()}: Mean = {value['mean']:.2f}, Std = {value['std']:.2f}, Positive = {value['positive_percent']:.1f}%")
        elif 'mean' in value:
            print(f"  {key.replace('_', ' ').title()}: Mean = {value['mean']:.2f}, Std = {value['std']:.2f}")
    
    print("\nPaired Comparisons:")
    for key, value in summary['paired'].items():
        print(f"  {key.replace('_', ' ').title()}: {value:.1f}%")
    
    # Print specific insights based on the data
    print("\nKey Insights:")
    
    if not prompt_only and 'sycophancy_gap' in summary['individual'] and summary['individual']['sycophancy_gap']:
        gap = summary['individual']['sycophancy_gap']['mean']
        positive_percent = summary['individual']['sycophancy_gap']['positive_percent']
        if gap > 0:
            print(f"  - Suggestive prompts increase sycophancy by {gap:.2f} points on average ({positive_percent:.1f}% of cases)")
        else:
            print(f"  - Suggestive prompts unexpectedly decrease sycophancy by {-gap:.2f} points on average ({100-positive_percent:.1f}% of cases)")
    
    if 'improvement' in summary['individual'] and summary['individual']['improvement']:
        improvement = summary['individual']['improvement']['mean']
        positive_percent = summary['individual']['improvement']['positive_percent']
        if improvement < 0:  # Negative is good for reducing sycophancy
            print(f"  - Steering reduces sycophancy by {-improvement:.2f} points on average ({positive_percent:.1f}% of cases)")
        else:
            print(f"  - Steering unexpectedly increases sycophancy by {improvement:.2f} points on average ({100-positive_percent:.1f}% of cases)")
    
    if (not prompt_only and 
        'steered_sycophancy_gap' in summary['individual'] and summary['individual']['steered_sycophancy_gap'] and
        'sycophancy_gap' in summary['individual'] and summary['individual']['sycophancy_gap']):
        gap = summary['individual']['steered_sycophancy_gap']['mean']
        orig_gap = summary['individual']['sycophancy_gap']['mean']
        print(f"  - Steering {'reduces' if gap < orig_gap else 'increases'} the sycophancy gap from {orig_gap:.2f} to {gap:.2f} points")
    
    print("\n" + "="*30 + "\n")


def save_results(responses, summary, input_file, args):
    """
    Save judged responses and summary to a JSON file.
    
    Args:
        responses (list): List of response dictionaries with judge results
        summary (dict): Summary statistics
        input_file (str): Original input file path
        args (Namespace): Command line arguments
        
    Returns:
        str: Path to the output file
    """
    # Create results folder if it doesn't exist
    Path(args.results_folder).mkdir(parents=True, exist_ok=True)
    
    # Extract input filename without extension
    input_base = os.path.basename(input_file)
    input_name = os.path.splitext(input_base)[0]
    
    # Create timestamp for unique filename
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Construct filename
    filename = f"{args.results_folder}/judge_{input_name}_{timestamp}.json"
    
    # Prepare data to save
    output_data = {
        'input_file': input_file,
        'judge_settings': vars(args),
        'summary': summary,
        'responses': responses
    }
    
    # Save to file - convert to serializable format
    with open(filename, 'w') as f:
        json.dump(output_data, f, indent=2, default=lambda o: float(o) if isinstance(o, np.number) else o)
    
    print(f"Saved judge results to {filename}")
    return filename


def main():
    parser = argparse.ArgumentParser(description="Judge responses for sycophancy evaluation")
    
    # Input arguments
    parser.add_argument("--input_file", type=str, required=True, help="Path to the JSON file with responses")
    parser.add_argument("--prompt_only", action="store_true", help="Whether this is prompt-only mode")
    
    # Judge arguments
    parser.add_argument("--openai_model", type=str, default="gpt-4o-mini", help="OpenAI model for judging")
    parser.add_argument("--api_key", type=str, default=None, help="OpenAI API key (uses env var if not provided)")
    
    # Evaluation mode
    parser.add_argument("--evaluate_single", action="store_true", help="Evaluate responses individually")
    parser.add_argument("--evaluate_paired", action="store_true", help="Evaluate responses in pairs")
    
    # Output arguments
    parser.add_argument("--results_folder", type=str, default="results/", help="Folder to save results")
    
    args = parser.parse_args()
    
    # If neither evaluation mode is specified, do both
    if not args.evaluate_single and not args.evaluate_paired:
        args.evaluate_single = True
        args.evaluate_paired = True
    
    # Setup
    setup_environment()
    
    # Load responses
    settings, responses = load_responses(args.input_file)
    
    # Check if this is prompt-only mode
    if args.prompt_only is None:
        # Try to infer from the data
        first_response = responses[0] if responses else {}
        args.prompt_only = 'base_prompt' not in first_response
    
    # Initialize OpenAI client
    client = initialize_openai_client(args.api_key)
    
    # Judge responses
    if args.evaluate_single:
        responses = judge_responses_single(responses, client, args.openai_model, prompt_only=args.prompt_only)
    
    if args.evaluate_paired:
        responses = judge_responses_paired(responses, client, args.openai_model, prompt_only=args.prompt_only)
    
    # Compute summary statistics
    summary = compute_summary_statistics(responses, prompt_only=args.prompt_only)
    
    # Print summary
    print_summary(summary, prompt_only=args.prompt_only)
    
    # Save results
    output_file = save_results(responses, summary, args.input_file, args)
    
    print(f"Judging complete. Results saved to {output_file}")


if __name__ == "__main__":
    main()