#!/usr/bin/env python3
"""
Judge responses generated by feedback_steering.py for sycophancy evaluation.
"""

import os
import sys

# Move one directory up
parent_dir = os.path.dirname(os.getcwd())
sys.path.append(parent_dir)

import json
from pathlib import Path
import numpy as np

from openai import OpenAI


def setup_environment():
    """Set up the environment by adding parent directory to path."""
    parent_dir = os.path.dirname(os.getcwd())
    os.chdir(parent_dir)
    sys.path.append(parent_dir)
    print(f"Moved to parent directory: {parent_dir}")


def load_responses(json_file):
    """
    Load responses from a JSON file.
    
    Args:
        json_file (str): Path to the JSON file
        
    Returns:
        tuple: (settings, responses) tuple
    """
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    settings = data.get('settings', {})
    responses = data.get('responses', [])
    
    print(f"Loaded {len(responses)} responses from {json_file}")
    print(f"Settings: {settings}")
    
    return settings, responses


def initialize_openai_client(api_key=None):
    """
    Initialize the OpenAI client with the provided API key.
    
    Args:
        api_key (str, optional): OpenAI API key. If None, uses environment variable.
        
    Returns:
        OpenAI: OpenAI client
    """
    if api_key is None:
        api_key = os.environ.get("OPENAI_API_KEY", "")
        
    if not api_key:
        raise ValueError("No OpenAI API key provided or found in environment")
        
    return OpenAI(api_key=api_key)


def create_score_stats(scores):
    """
    Create consistent statistics dictionary for a list of scores.
    
    Args:
        scores (list): List of numeric scores
        
    Returns:
        dict: Dictionary with consistent statistics
    """
    if not scores:
        return None
    
    return {
        'mean': float(np.mean(scores)),
        'std': float(np.std(scores)),
        'min': float(np.min(scores)),
        'max': float(np.max(scores)),
        'count': len(scores)
    }


def save_results(responses, summary, input_file, args):
    """
    Save judged responses and summary to a JSON file.
    
    Args:
        responses (list): List of response dictionaries with judge results
        summary (dict): Summary statistics
        input_file (str): Original input file path
        args (Namespace): Command line arguments
        
    Returns:
        str: Path to the output file
    """
    # Create results folder if it doesn't exist
    Path(args.results_folder).mkdir(parents=True, exist_ok=True)
    
    # Extract input filename without extension
    input_base = os.path.basename(input_file)
    input_name = os.path.splitext(input_base)[0]
    
    # Construct base filename
    base_filename = os.path.join(args.results_folder, f"judge_{input_name}_v")
    
    # Determine the next version number
    version = 1
    while os.path.exists(f"{base_filename}{version}.json"):
        version += 1
    
    # Final filename with version
    filename = f"{base_filename}{version}.json"
    
    # Prepare data to save
    output_data = {
        'input_file': input_file,
        'judge_settings': vars(args),
        'summary': summary,
        'responses': responses
    }
    
    # Save to file - convert to serializable format
    with open(filename, 'w') as f:
        json.dump(output_data, f, indent=2, default=lambda o: float(o) if isinstance(o, np.number) else o)
    
    print(f"Saved judge results to {filename}")
    return filename

