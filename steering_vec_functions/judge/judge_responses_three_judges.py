#!/usr/bin/env python3
"""
Run specific judging scenarios on responses generated by feedback_steering.py.
This script focuses on three specific evaluation scenarios:
1. Single response evaluation (non-steered only)
2. Paired evaluation (base vs suggestive)
3. Non-steered vs steered evaluation (for both base and suggestive)
"""

import os
import sys

# # Move one directory up
# parent_dir = os.path.dirname(os.getcwd())
# # os.chdir(parent_dir)
# sys.path.append(parent_dir)


import json
import argparse
import datetime
from pathlib import Path
import numpy as np
from tqdm import tqdm
from openai import OpenAI

# Import functions from judge_responses.py
from steering_vec_functions.judge.judge_responses import (
    # setup_environment,
    load_responses,
    initialize_openai_client,
    create_score_stats,
    save_results
)

# Import the evaluation functions directly
from steering_vec_functions.judge.openai_judge_json import evaluate_sycophancy_pair, evaluate_sycophancy_single


def judge_single_responses(responses, client, model_slug):
    """
    Judge base and suggestive responses individually (non-steered only).
    
    Args:
        responses (list): List of response dictionaries
        client (OpenAI): OpenAI client
        model_slug (str): Model to use for judging
        
    Returns:
        list: Updated responses with judge results
    """
    print("Running Scenario 1: Single response evaluation (non-steered only)")
    
    for item in tqdm(responses, desc="Judging individual responses"):
        result_single = {}
        
        # Only process items with both base and suggestive prompts
        if 'base_prompt' not in item or 'suggestive_prompt' not in item:
            continue
        
        # Judge the base response
        user_question = item['base_prompt']
        base_pair = {
            'user_question': user_question,
            'first_comment': item['base_response']
        }
        base_result = evaluate_sycophancy_single(client, model_slug, base_pair)
        result_single["base_response_score"] = base_result['sycophant_score_response']
        
        # Judge the suggestive response
        suggestive_pair = {
            'user_question': item['suggestive_prompt'],
            'first_comment': item['suggestive_response']
        }
        suggestive_result = evaluate_sycophancy_single(client, model_slug, suggestive_pair)
        result_single["suggestive_response_score"] = suggestive_result['sycophant_score_response']
        
        # Store results
        item['judge_single'] = result_single
    
    return responses


def judge_paired_base_vs_suggestive(responses, client, model_slug):
    """
    Judge base vs suggestive responses in pairs.
    
    Args:
        responses (list): List of response dictionaries
        client (OpenAI): OpenAI client
        model_slug (str): Model to use for judging
        
    Returns:
        list: Updated responses with judge results
    """
    print("Running Scenario 2: Paired evaluation (base vs suggestive)")
    
    for item in tqdm(responses, desc="Judging base vs suggestive pairs"):
        # Only process items with both base and suggestive prompts
        if 'base_prompt' not in item or 'suggestive_prompt' not in item:
            continue
        
        # Use the base prompt as the user question for consistency
        user_question = item['base_prompt']
        
        # Compare base with suggestive
        pair = {
            "first_comment": item['base_response'],
            "second_comment": item['suggestive_response'],
            "user_question": user_question
        }
        result = evaluate_sycophancy_pair(client, model_slug, pair)
        
        # Store results
        result_paired = {
            "base_response_score": result['response_a_sycophantic'],
            "suggestive_response_score": result['response_b_sycophantic'],
            'best_answer': result['preferred_response']
        }
        item['judge_base_vs_suggestive'] = result_paired
    
    return responses


def judge_steered_vs_unsteered(responses, client, model_slug):
    """
    Judge non-steered vs steered responses for both base and suggestive prompts.
    
    Args:
        responses (list): List of response dictionaries
        client (OpenAI): OpenAI client
        model_slug (str): Model to use for judging
        
    Returns:
        list: Updated responses with judge results
    """
    print("Running Scenario 3: Non-steered vs steered evaluation")
    
    for item in tqdm(responses, desc="Judging steered vs non-steered pairs"):
        # Skip if steered responses are not available
        if 'base_steered_response' not in item or 'suggestive_steered_response' not in item:
            continue
        
        # Compare base with base_steered
        base_pair = {
            "first_comment": item['base_response'],
            "second_comment": item['base_steered_response'],
            "user_question": item['base_prompt']
        }
        base_result = evaluate_sycophancy_pair(client, model_slug, base_pair, response_type="steered")
        
        # Store base comparison results
        base_paired = {
            "base_response_score": base_result['response_a_sycophantic'],
            "base_steered_response_score": base_result['response_b_sycophantic'],
            'best_answer': base_result['preferred_response']
        }
        item['judge_base_steered_pair'] = base_paired
        
        # Compare suggestive with suggestive_steered
        suggestive_pair = {
            "first_comment": item['suggestive_response'],
            "second_comment": item['suggestive_steered_response'],
            "user_question": item['suggestive_prompt']
        }
        suggestive_result = evaluate_sycophancy_pair(client, model_slug, suggestive_pair, response_type="steered")
        
        # Store suggestive comparison results
        suggestive_paired = {
            "suggestive_response_score": suggestive_result['response_a_sycophantic'],
            "suggestive_steered_response_score": suggestive_result['response_b_sycophantic'],
            'best_answer': suggestive_result['preferred_response']
        }
        item['judge_suggestive_steered_pair'] = suggestive_paired
    
    return responses


def compute_scenario_statistics(responses):
    """
    Compute statistics for each judging scenario.
    
    Args:
        responses (list): List of response dictionaries with judge results
        
    Returns:
        dict: Summary statistics for each scenario
    """

    summary = {
        'scenario1_single': {},
        'scenario2_paired': {},
        'scenario3_steered_pairs': {}
    }
    
    # Scenario 1: Single response evaluation
    base_scores = [r['judge_single'].get('base_response_score', 0) for r in responses 
                  if 'judge_single' in r and 'base_response_score' in r['judge_single']]
    
    suggestive_scores = [r['judge_single'].get('suggestive_response_score', 0) for r in responses 
                        if 'judge_single' in r and 'suggestive_response_score' in r['judge_single']]
    
    if base_scores:
        summary['scenario1_single']['base'] = create_score_stats(base_scores)
    
    if suggestive_scores:
        summary['scenario1_single']['suggestive'] = create_score_stats(suggestive_scores)
    
    # Calculate complete comparison statistics for single evaluation
    if base_scores and suggestive_scores and len(base_scores) == len(suggestive_scores):
        total_comparisons = len(base_scores)
        
        # Count occurrences of each outcome
        suggestive_higher = sum(1 for suggestive, base in zip(suggestive_scores, base_scores) if suggestive > base)
        scores_equal = sum(1 for suggestive, base in zip(suggestive_scores, base_scores) if suggestive == base)
        base_higher = sum(1 for suggestive, base in zip(suggestive_scores, base_scores) if suggestive < base)
        
        # Calculate percentages
        suggestive_higher_percent = (suggestive_higher / total_comparisons) * 100
        scores_equal_percent = (scores_equal / total_comparisons) * 100
        base_higher_percent = (base_higher / total_comparisons) * 100
        
        # Calculate the sycophancy gap statistics
        gaps = [suggestive - base for suggestive, base in zip(suggestive_scores, base_scores)]
        gap_stats = create_score_stats(gaps)
        
        if gap_stats:
            # Store all comparison statistics in sycophancy_gap
            summary['scenario1_single']['sycophancy_gap'] = gap_stats
            summary['scenario1_single']['sycophancy_gap'].update({
                'suggestive_higher_percent': suggestive_higher_percent,
                'scores_equal_percent': scores_equal_percent,
                'base_higher_percent': base_higher_percent
            })

    # summary = {
    #     'scenario1_single': {},
    #     'scenario2_paired': {},
    #     'scenario3_steered_pairs': {}
    # }
    
    # # Scenario 1: Single response evaluation
    # base_scores = [r['judge_single'].get('base_response_score', 0) for r in responses 
    #               if 'judge_single' in r and 'base_response_score' in r['judge_single']]
    
    # suggestive_scores = [r['judge_single'].get('suggestive_response_score', 0) for r in responses 
    #                     if 'judge_single' in r and 'suggestive_response_score' in r['judge_single']]
    
    # if base_scores:
    #     summary['scenario1_single']['base'] = create_score_stats(base_scores)
    
    # if suggestive_scores:
    #     summary['scenario1_single']['suggestive'] = create_score_stats(suggestive_scores)
    
    # Calculate sycophancy gap for single evaluation
    # if base_scores and suggestive_scores and len(base_scores) == len(suggestive_scores):
    #     gaps = [suggestive - base for suggestive, base in zip(suggestive_scores, base_scores)]
    #     gap_stats = create_score_stats(gaps)
    #     if gap_stats:
    #         gap_stats['positive_percent'] = sum(1 for gap in gaps if gap > 0) / len(gaps) * 100
    #         summary['scenario1_single']['sycophancy_gap'] = gap_stats
    
    # Scenario 2: Paired base vs suggestive
    paired_items = [r for r in responses if 'judge_base_vs_suggestive' in r]
    
    if paired_items:
        # Extract scores
        base_paired_scores = [r['judge_base_vs_suggestive']['base_response_score'] for r in paired_items]
        suggestive_paired_scores = [r['judge_base_vs_suggestive']['suggestive_response_score'] for r in paired_items]
        
        summary['scenario2_paired']['base'] = create_score_stats(base_paired_scores)
        summary['scenario2_paired']['suggestive'] = create_score_stats(suggestive_paired_scores)
        
        # Calculate preference statistics
        suggestive_preferred = [r['judge_base_vs_suggestive']['best_answer'] == 'B' for r in paired_items]
        summary['scenario2_paired']['suggestive_preferred_percent'] = sum(suggestive_preferred) / len(suggestive_preferred) * 100
        
        # Calculate sycophancy gap for paired evaluation
        if len(base_paired_scores) == len(suggestive_paired_scores):
            paired_gaps = [suggestive - base for suggestive, base in zip(suggestive_paired_scores, base_paired_scores)]
            paired_gap_stats = create_score_stats(paired_gaps)
            if paired_gap_stats:
                paired_gap_stats['positive_percent'] = sum(1 for gap in paired_gaps if gap > 0) / len(paired_gaps) * 100
                summary['scenario2_paired']['sycophancy_gap'] = paired_gap_stats
    
    # Scenario 3: Steered vs Non-steered
    base_steered_items = [r for r in responses if 'judge_base_steered_pair' in r]
    suggestive_steered_items = [r for r in responses if 'judge_suggestive_steered_pair' in r]
    
    # Base steered statistics
    if base_steered_items:
        base_scores = [r['judge_base_steered_pair']['base_response_score'] for r in base_steered_items]
        base_steered_scores = [r['judge_base_steered_pair']['base_steered_response_score'] for r in base_steered_items]
        
        summary['scenario3_steered_pairs']['base'] = create_score_stats(base_scores)
        summary['scenario3_steered_pairs']['base_steered'] = create_score_stats(base_steered_scores)
        
        # Calculate preference statistics
        steered_preferred = [r['judge_base_steered_pair']['best_answer'] == 'B' for r in base_steered_items]
        summary['scenario3_steered_pairs']['base_steered_preferred_percent'] = sum(steered_preferred) / len(steered_preferred) * 100
        
        # Calculate improvement
        if len(base_scores) == len(base_steered_scores):
            improvements = [steered - original for steered, original in zip(base_steered_scores, base_scores)]
            improvement_stats = create_score_stats(improvements)
            if improvement_stats:
                # For sycophancy, lower scores are better, so we want negative improvements
                improvement_stats['positive_percent'] = sum(1 for imp in improvements if imp < 0) / len(improvements) * 100
                summary['scenario3_steered_pairs']['base_improvement'] = improvement_stats
    
    # Suggestive steered statistics
    if suggestive_steered_items:
        suggestive_scores = [r['judge_suggestive_steered_pair']['suggestive_response_score'] for r in suggestive_steered_items]
        suggestive_steered_scores = [r['judge_suggestive_steered_pair']['suggestive_steered_response_score'] for r in suggestive_steered_items]
        
        summary['scenario3_steered_pairs']['suggestive'] = create_score_stats(suggestive_scores)
        summary['scenario3_steered_pairs']['suggestive_steered'] = create_score_stats(suggestive_steered_scores)
        
        # Calculate preference statistics
        steered_preferred = [r['judge_suggestive_steered_pair']['best_answer'] == 'B' for r in suggestive_steered_items]
        summary['scenario3_steered_pairs']['suggestive_steered_preferred_percent'] = sum(steered_preferred) / len(steered_preferred) * 100
        
        # Calculate improvement
        if len(suggestive_scores) == len(suggestive_steered_scores):
            improvements = [steered - original for steered, original in zip(suggestive_steered_scores, suggestive_scores)]
            improvement_stats = create_score_stats(improvements)
            if improvement_stats:
                # For sycophancy, lower scores are better, so we want negative improvements
                improvement_stats['positive_percent'] = sum(1 for imp in improvements if imp < 0) / len(improvements) * 100
                summary['scenario3_steered_pairs']['suggestive_improvement'] = improvement_stats
    
    # Compare non-steered suggestive vs non-steered base
    common_items = [r for r in responses if 'judge_base_steered_pair' in r and 'judge_suggestive_steered_pair' in r]
    
    if common_items:
        # Extract non-steered scores for comparison
        base_scores = [r['judge_base_steered_pair']['base_response_score'] for r in common_items]
        suggestive_scores = [r['judge_suggestive_steered_pair']['suggestive_response_score'] for r in common_items]
        
        # Calculate all three comparison outcomes
        if len(base_scores) == len(suggestive_scores):
            total_comparisons = len(base_scores)
            
            # Count occurrences of each outcome
            suggestive_higher = sum(1 for suggestive, base in zip(suggestive_scores, base_scores) if suggestive > base)
            scores_equal = sum(1 for suggestive, base in zip(suggestive_scores, base_scores) if suggestive == base)
            base_higher = sum(1 for suggestive, base in zip(suggestive_scores, base_scores) if suggestive < base)
            
            # Calculate percentages
            suggestive_higher_percent = (suggestive_higher / total_comparisons) * 100
            scores_equal_percent = (scores_equal / total_comparisons) * 100
            base_higher_percent = (base_higher / total_comparisons) * 100
            
            # Calculate the average difference
            score_diffs = [suggestive - base for suggestive, base in zip(suggestive_scores, base_scores)]
            diff_stats = create_score_stats(score_diffs)
            
            # Store the results
            if 'non_steered_comparison' not in summary['scenario3_steered_pairs']:
                summary['scenario3_steered_pairs']['non_steered_comparison'] = {}
                
            summary['scenario3_steered_pairs']['non_steered_comparison'] = {
                'suggestive_higher_percent': suggestive_higher_percent,
                'scores_equal_percent': scores_equal_percent,
                'base_higher_percent': base_higher_percent,
                'mean_difference': diff_stats['mean'] if diff_stats else None,
                'std_difference': diff_stats['std'] if diff_stats else None
            }
    
    return summary


def print_scenario_summary(summary):
    """
    Print summary statistics for each scenario in a readable format.
    
    Args:
        summary (dict): Summary statistics for each scenario
    """
    print("\n=========== SCENARIO SUMMARIES ===========\n")
    
    # Scenario 1: Single response evaluation
    print("Scenario 1: Single Response Evaluation")
    print("-" * 40)
    
    if 'scenario1_single' in summary and summary['scenario1_single']:
        for key, value in summary['scenario1_single'].items():
            if not value:
                continue
                
            if key == 'sycophancy_gap':
                print(f"  Sycophancy Gap: Mean = {value['mean']:.2f}, Std = {value['std']:.2f}")
                print(f"  Comparison breakdown:")
                print(f"    Suggestive score higher than base: {value['suggestive_higher_percent']:.1f}%")
                print(f"    Suggestive score equal to base: {value['scores_equal_percent']:.1f}%")
                print(f"    Suggestive score lower than base: {value['base_higher_percent']:.1f}%")
            else:
                print(f"  {key.replace('_', ' ').title()}: Mean = {value['mean']:.2f}, Std = {value['std']:.2f}")
    else:
        print("  No data available for Scenario 1")
    
    print("\n")
    
    # Scenario 2: Paired base vs suggestive
    print("Scenario 2: Paired Base vs Suggestive Evaluation")
    print("-" * 40)
    
    if 'scenario2_paired' in summary and summary['scenario2_paired']:
        for key, value in summary['scenario2_paired'].items():
            if not value:
                continue
                
            if key == 'sycophancy_gap':
                print(f"  Sycophancy Gap: Mean = {value['mean']:.2f}, Std = {value['std']:.2f}")
                print(f"  Cases with higher suggestive score: {value['positive_percent']:.1f}%")
            elif key == 'suggestive_preferred_percent':
                print(f"  Suggestive responses preferred: {value:.1f}%")
            else:
                print(f"  {key.replace('_', ' ').title()}: Mean = {value['mean']:.2f}, Std = {value['std']:.2f}")
    else:
        print("  No data available for Scenario 2")
    
    print("\n")
    

    # Scenario 3: Steered vs Non-steered
    print("Scenario 3: Steered vs Non-steered Evaluation")
    print("-" * 40)
    
    if 'scenario3_steered_pairs' in summary and summary['scenario3_steered_pairs']:
        # Base responses
        print("  Base Responses:")
        if 'base' in summary['scenario3_steered_pairs'] and 'base_steered' in summary['scenario3_steered_pairs']:
            base = summary['scenario3_steered_pairs']['base']
            base_steered = summary['scenario3_steered_pairs']['base_steered']
            print(f"    Original: Mean = {base['mean']:.2f}, Std = {base['std']:.2f}")
            print(f"    Steered:  Mean = {base_steered['mean']:.2f}, Std = {base_steered['std']:.2f}")
            
            if 'base_steered_preferred_percent' in summary['scenario3_steered_pairs']:
                pref = summary['scenario3_steered_pairs']['base_steered_preferred_percent']
                print(f"    Steered responses preferred: {pref:.1f}%")
            
            if 'base_improvement' in summary['scenario3_steered_pairs']:
                imp = summary['scenario3_steered_pairs']['base_improvement']
                if imp['mean'] < 0:
                    print(f"    Steering reduced sycophancy by {-imp['mean']:.2f} points")
                    print(f"    Improvement in {imp['positive_percent']:.1f}% of cases")
                else:
                    print(f"    Steering increased sycophancy by {imp['mean']:.2f} points")
                    print(f"    Worsening in {100-imp['positive_percent']:.1f}% of cases")
        
        # Suggestive responses
        print("\n  Suggestive Responses:")
        if 'suggestive' in summary['scenario3_steered_pairs'] and 'suggestive_steered' in summary['scenario3_steered_pairs']:
            suggestive = summary['scenario3_steered_pairs']['suggestive']
            suggestive_steered = summary['scenario3_steered_pairs']['suggestive_steered']
            print(f"    Original: Mean = {suggestive['mean']:.2f}, Std = {suggestive['std']:.2f}")
            print(f"    Steered:  Mean = {suggestive_steered['mean']:.2f}, Std = {suggestive_steered['std']:.2f}")
            
            if 'suggestive_steered_preferred_percent' in summary['scenario3_steered_pairs']:
                pref = summary['scenario3_steered_pairs']['suggestive_steered_preferred_percent']
                print(f"    Steered responses preferred: {pref:.1f}%")
            
            if 'suggestive_improvement' in summary['scenario3_steered_pairs']:
                imp = summary['scenario3_steered_pairs']['suggestive_improvement']
                if imp['mean'] < 0:
                    print(f"    Steering reduced sycophancy by {-imp['mean']:.2f} points")
                    print(f"    Improvement in {imp['positive_percent']:.1f}% of cases")
                else:
                    print(f"    Steering increased sycophancy by {imp['mean']:.2f} points")
                    print(f"    Worsening in {100-imp['positive_percent']:.1f}% of cases")


        # Print non-steered comparison results
        print("\n  Non-steered Comparison (Suggestive vs Base):")
        if 'non_steered_comparison' in summary['scenario3_steered_pairs']:
            comparison = summary['scenario3_steered_pairs']['non_steered_comparison']
            print(f"    Non-steered suggestive higher than non-steered base: {comparison['suggestive_higher_percent']:.1f}%")
            print(f"    Non-steered suggestive equal to non-steered base: {comparison['scores_equal_percent']:.1f}%")
            print(f"    Non-steered suggestive lower than non-steered base: {comparison['base_higher_percent']:.1f}%")
            print(f"    Average difference (suggestive - base): {comparison['mean_difference']:.2f}")
            print(f"    Standard deviation of difference: {comparison['std_difference']:.2f}")

    else:
        print("  No data available for Scenario 3")
    
    print("\n")
    
    # Overall insights
    print("Overall Insights")
    print("-" * 40)
    
    # Compare single vs paired evaluation
    if ('scenario1_single' in summary and 'scenario2_paired' in summary and
        'sycophancy_gap' in summary['scenario1_single'] and 'sycophancy_gap' in summary['scenario2_paired']):
        
        single_gap = summary['scenario1_single']['sycophancy_gap']['mean']
        paired_gap = summary['scenario2_paired']['sycophancy_gap']['mean']
        
        print(f"  Single vs Paired Evaluation:")
        print(f"    Single evaluation sycophancy gap: {single_gap:.2f}")
        print(f"    Paired evaluation sycophancy gap: {paired_gap:.2f}")
        print(f"    Difference: {abs(single_gap - paired_gap):.2f}")
        
        if abs(single_gap - paired_gap) > 0.5:
            print(f"    NOTE: There is a substantial difference between single and paired evaluation methods")
    
    # Compare steering effectiveness for base vs suggestive
    if ('scenario3_steered_pairs' in summary and
        'base_improvement' in summary['scenario3_steered_pairs'] and
        'suggestive_improvement' in summary['scenario3_steered_pairs']):
        
        base_imp = summary['scenario3_steered_pairs']['base_improvement']['mean']
        suggestive_imp = summary['scenario3_steered_pairs']['suggestive_improvement']['mean']
        
        print(f"\n  Steering Effectiveness:")
        print(f"    Base response improvement: {-base_imp:.2f}")
        print(f"    Suggestive response improvement: {-suggestive_imp:.2f}")
        
        if abs(base_imp) > abs(suggestive_imp):
            print(f"    Steering was more effective for base responses")
        else:
            print(f"    Steering was more effective for suggestive responses")
    
    print("\n" + "="*50 + "\n")


def main():
    parser = argparse.ArgumentParser(description="Run specific judging scenarios on responses")
    
    # Input arguments
    parser.add_argument("--input_file", type=str, required=True, help="Path to the JSON file with responses")
    
    # Judge arguments
    parser.add_argument("--openai_model", type=str, default="gpt-4o-mini", help="OpenAI model for judging")
    parser.add_argument("--api_key", type=str, default=None, help="OpenAI API key (uses env var if not provided)")
    
    # Scenario selection
    parser.add_argument("--run_scenario1", action="store_true", help="Run Scenario 1: Single response evaluation")
    parser.add_argument("--run_scenario2", action="store_true", help="Run Scenario 2: Paired base vs suggestive")
    parser.add_argument("--run_scenario3", action="store_true", help="Run Scenario 3: Steered vs non-steered")
    parser.add_argument("--run_all", action="store_true", help="Run all three scenarios")
    
    # Output arguments
    parser.add_argument("--results_folder", type=str, default="results/judge_results/", help="Folder to save results")
    
    args = parser.parse_args()
    
    # If no specific scenario is selected, run all
    if not any([args.run_scenario1, args.run_scenario2, args.run_scenario3, args.run_all]):
        args.run_all = True
    
    # Set all scenarios if run_all is selected
    if args.run_all:
        args.run_scenario1 = True
        args.run_scenario2 = True
        args.run_scenario3 = True
    
    # Setup
    # setup_environment()
    
    # Load responses
    settings, responses = load_responses(args.input_file)
    
    # Initialize OpenAI client
    client = initialize_openai_client(args.api_key)
    
    # Run selected scenarios
    if args.run_scenario1:
        responses = judge_single_responses(responses, client, args.openai_model)
    
    if args.run_scenario2:
        responses = judge_paired_base_vs_suggestive(responses, client, args.openai_model)
    
    if args.run_scenario3:
        responses = judge_steered_vs_unsteered(responses, client, args.openai_model)
    
    # Compute and print summary statistics
    summary = compute_scenario_statistics(responses)
    print_scenario_summary(summary)
    
    # Save results
    output_file = save_results(responses, summary, args.input_file, args)
    
    print(f"All scenarios completed. Results saved to {output_file}")


if __name__ == "__main__":
    main()