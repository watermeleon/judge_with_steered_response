#!/usr/bin/env python3
"""
Run specific judging scenarios on responses generated by feedback_steering.py.
This script evaluates three scenarios:
1. Single response evaluation (non-steered only)
2. Paired evaluation (base vs suggestive)
3. Non-steered vs steered evaluation (for both base and suggestive)
"""

import json
import argparse
import datetime
from pathlib import Path
import numpy as np
from tqdm import tqdm
from openai import OpenAI
import wandb

# Import functions from judge_responses.py
from steering_vec_functions.judge.judge_responses import (
    load_responses,
    initialize_openai_client,
    create_score_stats,
    save_results
)

# Import the improved evaluation functions
from steering_vec_functions.judge.openai_judge_json import (
    evaluate_response_pair,
    evaluate_single_response
)


def run_evaluations(responses, client, model_slug, data_type="manipulation"):
    """
    Run all three evaluation scenarios on the responses.
    
    Args:
        responses (list): List of response dictionaries
        client (OpenAI): OpenAI client
        model_slug (str): Model to use for judging
        data_type (str): Type of evaluation ("sycophancy" or "manipulation")
        
    Returns:
        list: Updated responses with judge results
    """
    print(f"Running evaluations for {data_type}")
    
    for item in tqdm(responses, desc="Processing responses"):
        # Scenario 1: Single response evaluation
        if 'base_prompt' in item and 'suggestive_prompt' in item:
            # Evaluate base response
            base_single = evaluate_single_response(
                client, model_slug, 
                {
                    'user_question': item['base_prompt'],
                    'first_comment': item['base_response']
                },
                data_type=data_type
            )
            
            # Evaluate suggestive response
            suggestive_single = evaluate_single_response(
                client, model_slug,
                {
                    'user_question': item['suggestive_prompt'],
                    'first_comment': item['suggestive_response']
                },
                data_type=data_type
            )
            
            # Store results
            item['judge_single'] = {
                'base': base_single['response_A'],
                'suggestive': suggestive_single['response_A']
            }
            
            # Scenario 2: Paired evaluation (base vs suggestive)
            paired_result = evaluate_response_pair(
                client, model_slug,
                {
                    'user_question': item['base_prompt'],
                    'first_comment': item['base_response'],
                    'second_comment': item['suggestive_response']
                },
                response_type="pair",
                data_type=data_type
            )
            
            item['judge_base_vs_suggestive'] = paired_result
        
        # Scenario 3: Steered vs non-steered evaluation
        if 'base_steered_response' in item and 'suggestive_steered_response' in item:
            # Base vs base_steered
            base_steered_result = evaluate_response_pair(
                client, model_slug,
                {
                    'user_question': item['base_prompt'],
                    'first_comment': item['base_response'],
                    'second_comment': item['base_steered_response']
                },
                response_type="steer",
                data_type=data_type
            )
            
            item['judge_base_steered_pair'] = base_steered_result
            
            # Suggestive vs suggestive_steered
            suggestive_steered_result = evaluate_response_pair(
                client, model_slug,
                {
                    'user_question': item['suggestive_prompt'],
                    'first_comment': item['suggestive_response'],
                    'second_comment': item['suggestive_steered_response']
                },
                response_type="steer",
                data_type=data_type
            )
            
            item['judge_suggestive_steered_pair'] = suggestive_steered_result
    
    return responses


def compute_statistics_for_metric(responses, metric_key="metric_score"):
    """
    Compute statistics for a specific metric across all scenarios.
    
    Args:
        responses (list): List of response dictionaries with judge results
        metric_key (str): Key for the metric to analyze
        
    Returns:
        dict: Summary statistics for each scenario
    """
    summary = {
        'scenario1_single': {},
        'scenario2_paired': {},
        'scenario3_steered_pairs': {}
    }
    
    # Scenario 1: Single response evaluation
    single_items = [r for r in responses if 'judge_single' in r]
    
    if single_items:
        base_scores = [r['judge_single']['base'][metric_key] for r in single_items 
                      if r['judge_single']['base'][metric_key] >= 0]
        
        suggestive_scores = [r['judge_single']['suggestive'][metric_key] for r in single_items 
                           if r['judge_single']['suggestive'][metric_key] >= 0]
        
        if base_scores:
            summary['scenario1_single']['base'] = create_score_stats(base_scores)
        
        if suggestive_scores:
            summary['scenario1_single']['suggestive'] = create_score_stats(suggestive_scores)
        
        # Calculate comparison statistics
        if base_scores and suggestive_scores and len(base_scores) == len(suggestive_scores):
            gaps = [s - b for s, b in zip(suggestive_scores, base_scores)]
            gap_stats = create_score_stats(gaps)
            
            higher_count = sum(1 for gap in gaps if gap > 0)
            equal_count = sum(1 for gap in gaps if gap == 0)
            lower_count = sum(1 for gap in gaps if gap < 0)
            total = len(gaps)
            
            gap_stats.update({
                'suggestive_higher_percent': (higher_count / total) * 100,
                'scores_equal_percent': (equal_count / total) * 100,
                'base_higher_percent': (lower_count / total) * 100
            })
            
            summary['scenario1_single']['gap'] = gap_stats
    
    # Scenario 2: Paired evaluation
    paired_items = [r for r in responses if 'judge_base_vs_suggestive' in r]
    
    if paired_items:
        base_scores = [r['judge_base_vs_suggestive']['response_A'][metric_key] for r in paired_items]
        suggestive_scores = [r['judge_base_vs_suggestive']['response_B'][metric_key] for r in paired_items]
        preferred_b = [r['judge_base_vs_suggestive']['preferred_response'] == 'B' for r in paired_items]
        
        summary['scenario2_paired']['base'] = create_score_stats(base_scores)
        summary['scenario2_paired']['suggestive'] = create_score_stats(suggestive_scores)
        summary['scenario2_paired']['suggestive_preferred_percent'] = (sum(preferred_b) / len(preferred_b)) * 100
        
        # Calculate gap
        gaps = [s - b for s, b in zip(suggestive_scores, base_scores)]
        gap_stats = create_score_stats(gaps)
        gap_stats['positive_percent'] = sum(1 for gap in gaps if gap > 0) / len(gaps) * 100
        summary['scenario2_paired']['gap'] = gap_stats
    
    # Scenario 3: Steered evaluation
    base_steered_items = [r for r in responses if 'judge_base_steered_pair' in r]
    suggestive_steered_items = [r for r in responses if 'judge_suggestive_steered_pair' in r]
    
    # Process base steered
    if base_steered_items:
        base_scores = [r['judge_base_steered_pair']['response_A'][metric_key] for r in base_steered_items]
        steered_scores = [r['judge_base_steered_pair']['response_B'][metric_key] for r in base_steered_items]
        preferred_b = [r['judge_base_steered_pair']['preferred_response'] == 'B' for r in base_steered_items]
        
        summary['scenario3_steered_pairs']['base'] = create_score_stats(base_scores)
        summary['scenario3_steered_pairs']['base_steered'] = create_score_stats(steered_scores)
        summary['scenario3_steered_pairs']['base_steered_preferred_percent'] = (sum(preferred_b) / len(preferred_b)) * 100
        
        # Calculate improvement
        improvements = [s - b for s, b in zip(steered_scores, base_scores)]
        imp_stats = create_score_stats(improvements)
        imp_stats['improvement_percent'] = sum(1 for imp in improvements if imp < 0) / len(improvements) * 100
        summary['scenario3_steered_pairs']['base_improvement'] = imp_stats
    
    # Process suggestive steered
    if suggestive_steered_items:
        suggestive_scores = [r['judge_suggestive_steered_pair']['response_A'][metric_key] for r in suggestive_steered_items]
        steered_scores = [r['judge_suggestive_steered_pair']['response_B'][metric_key] for r in suggestive_steered_items]
        preferred_b = [r['judge_suggestive_steered_pair']['preferred_response'] == 'B' for r in suggestive_steered_items]
        
        summary['scenario3_steered_pairs']['suggestive'] = create_score_stats(suggestive_scores)
        summary['scenario3_steered_pairs']['suggestive_steered'] = create_score_stats(steered_scores)
        summary['scenario3_steered_pairs']['suggestive_steered_preferred_percent'] = (sum(preferred_b) / len(preferred_b)) * 100
        
        # Calculate improvement
        improvements = [s - b for s, b in zip(steered_scores, suggestive_scores)]
        imp_stats = create_score_stats(improvements)
        imp_stats['improvement_percent'] = sum(1 for imp in improvements if imp < 0) / len(improvements) * 100
        summary['scenario3_steered_pairs']['suggestive_improvement'] = imp_stats
    
    # Compare non-steered suggestive vs non-steered base
    common_items = [r for r in responses if 'judge_base_steered_pair' in r and 'judge_suggestive_steered_pair' in r]
    
    if common_items:
        # Extract non-steered scores for comparison
        base_non_steered = [r['judge_base_steered_pair']['response_A'][metric_key] for r in common_items]
        suggestive_non_steered = [r['judge_suggestive_steered_pair']['response_A'][metric_key] for r in common_items]
        
        # Calculate comparison statistics
        if len(base_non_steered) == len(suggestive_non_steered):
            total_comparisons = len(base_non_steered)
            
            # Count occurrences of each outcome
            suggestive_higher = sum(1 for s, b in zip(suggestive_non_steered, base_non_steered) if s > b)
            scores_equal = sum(1 for s, b in zip(suggestive_non_steered, base_non_steered) if s == b)
            base_higher = sum(1 for s, b in zip(suggestive_non_steered, base_non_steered) if s < b)
            
            # Calculate percentages
            suggestive_higher_percent = (suggestive_higher / total_comparisons) * 100
            scores_equal_percent = (scores_equal / total_comparisons) * 100
            base_higher_percent = (base_higher / total_comparisons) * 100
            
            # Calculate the average difference
            score_diffs = [s - b for s, b in zip(suggestive_non_steered, base_non_steered)]
            diff_stats = create_score_stats(score_diffs)
            
            # Store the results
            summary['scenario3_steered_pairs']['non_steered_comparison'] = {
                'suggestive_higher_percent': suggestive_higher_percent,
                'scores_equal_percent': scores_equal_percent,
                'base_higher_percent': base_higher_percent,
                'mean_difference': diff_stats['mean'] if diff_stats else None,
                'std_difference': diff_stats['std'] if diff_stats else None
            }
    
    return summary


def print_metric_summary(summary, metric_name="manipulation"):
    """
    Print summary statistics for a specific metric.
    
    Args:
        summary (dict): Summary statistics for the metric
        metric_name (str): Name of the metric being analyzed
    """
    print(f"\n========== {metric_name.upper()} EVALUATION SUMMARY ==========\n")
    
    # Scenario 1
    print("Scenario 1: Single Response Evaluation")
    print("-" * 40)
    
    if summary['scenario1_single']:
        for key, value in summary['scenario1_single'].items():
            if key == 'gap':
                print(f"  {metric_name} Gap: Mean = {value['mean']:.2f}, Std = {value['std']:.2f}")
                print(f"  Comparison breakdown:")
                print(f"    Suggestive > Base: {value['suggestive_higher_percent']:.1f}%")
                print(f"    Suggestive = Base: {value['scores_equal_percent']:.1f}%")
                print(f"    Suggestive < Base: {value['base_higher_percent']:.1f}%")
            else:
                print(f"  {key.title()}: Mean = {value['mean']:.2f}, Std = {value['std']:.2f}")
    
    # Scenario 2
    print("\nScenario 2: Paired Evaluation")
    print("-" * 40)
    
    if summary['scenario2_paired']:
        for key, value in summary['scenario2_paired'].items():
            if key == 'gap':
                print(f"  {metric_name} Gap: Mean = {value['mean']:.2f}, Std = {value['std']:.2f}")
                print(f"    Higher suggestive: {value['positive_percent']:.1f}%")
            elif key == 'suggestive_preferred_percent':
                print(f"  Suggestive preferred: {value:.1f}%")
            else:
                print(f"  {key.title()}: Mean = {value['mean']:.2f}, Std = {value['std']:.2f}")
    
    # Scenario 3
    print("\nScenario 3: Steered Evaluation")
    print("-" * 40)
    
    if summary['scenario3_steered_pairs']:
        # Base responses
        if 'base' in summary['scenario3_steered_pairs']:
            print("  Base Responses:")
            print(f"    Original: Mean = {summary['scenario3_steered_pairs']['base']['mean']:.2f}")
            print(f"    Steered:  Mean = {summary['scenario3_steered_pairs']['base_steered']['mean']:.2f}")
            
            if 'base_improvement' in summary['scenario3_steered_pairs']:
                imp = summary['scenario3_steered_pairs']['base_improvement']
                print(f"    Change: {imp['mean']:.2f} ({imp['improvement_percent']:.1f}% improved)")
        
        # Suggestive responses
        if 'suggestive' in summary['scenario3_steered_pairs']:
            print("\n  Suggestive Responses:")
            print(f"    Original: Mean = {summary['scenario3_steered_pairs']['suggestive']['mean']:.2f}")
            print(f"    Steered:  Mean = {summary['scenario3_steered_pairs']['suggestive_steered']['mean']:.2f}")
            
            if 'suggestive_improvement' in summary['scenario3_steered_pairs']:
                imp = summary['scenario3_steered_pairs']['suggestive_improvement']
                print(f"    Change: {imp['mean']:.2f} ({imp['improvement_percent']:.1f}% improved)")
        
        # Non-steered comparison
        if 'non_steered_comparison' in summary['scenario3_steered_pairs']:
            print("\n  Non-steered Comparison (Suggestive vs Base):")
            comp = summary['scenario3_steered_pairs']['non_steered_comparison']
            print(f"    Suggestive > Base: {comp['suggestive_higher_percent']:.1f}%")
            print(f"    Suggestive = Base: {comp['scores_equal_percent']:.1f}%")
            print(f"    Suggestive < Base: {comp['base_higher_percent']:.1f}%")
            print(f"    Mean difference: {comp['mean_difference']:.2f}")
            print(f"    Std difference: {comp['std_difference']:.2f}")
    
    print("\n" + "="*50 + "\n")


def main():
    parser = argparse.ArgumentParser(description="Run evaluation scenarios on responses")
    
    # Input/output arguments
    parser.add_argument("--input_file", type=str, required=True, help="Path to JSON file with responses")
    parser.add_argument("--results_folder", type=str, default="results/judge_results/", help="Folder to save results")
    
    # Judge arguments
    parser.add_argument("--openai_model", type=str, default="gpt-4o-mini", help="OpenAI model for judging")
    parser.add_argument("--api_key", type=str, default=None, help="OpenAI API key")
    # parser.add_argument("--data_type", type=str, default="manipulation", 
    #                    choices=["sycophancy", "manipulation"], help="Type of evaluation")
    
    # Scenario selection
    parser.add_argument("--run_scenario1", action="store_true", help="Run single response evaluation")
    parser.add_argument("--run_scenario2", action="store_true", help="Run paired evaluation")
    parser.add_argument("--run_scenario3", action="store_true", help="Run steered evaluation")
    parser.add_argument("--run_all", action="store_true", help="Run all scenarios")
    
    args = parser.parse_args()

    wandb.init(project="judge_responses", mode="online")
    wandb.config.update(vars(args))
    
    # Default to running all scenarios
    if not any([args.run_scenario1, args.run_scenario2, args.run_scenario3, args.run_all]):
        args.run_all = True
    
    if args.run_all:
        args.run_scenario1 = args.run_scenario2 = args.run_scenario3 = True
    
    if "/" not in args.input_file:
        base_path = "results/responses/"
        print("File directly provided so am setting base path to: " + repr(base_path))
        args.input_file = base_path + args.input_file
        
    # Load responses and initialize client
    settings, responses = load_responses(args.input_file)
    client = initialize_openai_client(args.api_key)

    data_type = settings["data_set"]
    print(f"# Data type found: {data_type}")
    
    responses = responses[:5]
    # Run evaluations
    responses = run_evaluations(responses, client, args.openai_model, data_type)
    
    # Compute statistics for metric score
    metric_summary = compute_statistics_for_metric(responses, "metric_score")
    print_metric_summary(metric_summary, data_type)
    
    # Compute statistics for correctness (if needed)
    correctness_summary = compute_statistics_for_metric(responses, "correctness")
    print_metric_summary(correctness_summary, "correctness")
    
    # Combine summaries for saving
    full_summary = {
        f"{data_type}_scores": metric_summary,
        "correctness_scores": correctness_summary
    }


    # Save results
    output_file = save_results(responses, full_summary, args.input_file, args)
    print(f"Results saved to {output_file}")


    from steering_vec_functions.visualizations.viz_judge_results import process_category_statistics, plot_category_comparison, display_comparison_table
    statistics = process_category_statistics(responses, cat_param='category_id', metric_name='metric_score')
    fig = plot_category_comparison(statistics)
    full_summary['statistics'] = statistics
    wandb.log({"single_steered_fig": wandb.Image(fig)})
    # full_summary['single_steered_fig'] = fig
    wandb.log(full_summary)
    table = display_comparison_table(metric_summary)
    score_count_table = wandb.Table(dataframe=table)
    wandb.log({"BaseSugg_Relative_Score_count": score_count_table})

    wandb.finish()
    
if __name__ == "__main__":
    main()